{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a394fa",
   "metadata": {},
   "source": [
    "# Final Course Group Project – Insurance Purchase Prediction\n",
    "\n",
    "**Course:** BZAN 6357 – Business Analytics with Python  \n",
    "**Project Type:** Supervised ML (Classification)  \n",
    "**Template generated:** 2025-10-30\n",
    "\n",
    "## Team\n",
    "- Aditya Boghara \n",
    "- Meghana\n",
    "\n",
    "## Deliverables\n",
    "Submit a single zip with:  \n",
    "1) This notebook (fully executed).  \n",
    "2) `my_prediction.csv` with **exactly** 3 columns: `id_new`, `probability`, `classification`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce3064",
   "metadata": {},
   "source": [
    "## 1) Introduction & Objective\n",
    "- **Background:** Cross-sell *car insurance* to existing medical policyholders.\n",
    "- **Objective:** Predict purchase probability (1=purchased, 0=not purchased) and classify Score data.\n",
    "- **Evaluation:** AUC-ROC and F1 score on held-out test; clarity and rigor of this notebook.\n",
    "- **Approach (summary):** Data prep → EDA → Modeling (baseline → tuned) → Evaluation → Score file export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc08fd9",
   "metadata": {},
   "source": [
    "## 2) Setup\n",
    "Fill in project constants and file paths if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a86f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Project constants ===\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2  # 20% test split\n",
    "N_FOLDS = 5      # 5- or 10-fold CV recommended\n",
    "\n",
    "# File names expected by the project\n",
    "TRAIN_FILE = 'bzan6357_insurance_3_TRAINING.csv'\n",
    "SCORE_FILE = 'bzan6357_insurance_3_SCORE.csv'\n",
    "SUBMIT_FILE = 'my_prediction.csv'  # must contain: id_new, probability, classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d3367",
   "metadata": {},
   "source": [
    "## 3) Imports\n",
    "Only add libraries you actually use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd95ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, roc_curve\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, \\\n",
    "                            precision_score, recall_score, f1_score, roc_auc_score,roc_curve "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f304c9",
   "metadata": {},
   "source": [
    "## 4) Data Load & Quick Audit\n",
    "If files are missing, you'll see a helpful message instead of a crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1747790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy\n",
      "0    16705\n",
      "1     3755\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "buy\n",
       "0    81.647116\n",
       "1    18.352884\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data (paths are already set above)\n",
    "df_train = pd.read_csv(\"bzan6357_insurance_3_TRAINING.csv\")\n",
    "df_score = pd.read_csv(\"bzan6357_insurance_3_SCORE.csv\")\n",
    "\n",
    "df_train.head()\n",
    "\n",
    "y = df_train['buy']\n",
    "\n",
    "print(y.value_counts())\n",
    "\n",
    "y.value_counts(normalize=True) * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be43733d",
   "metadata": {},
   "source": [
    "## 5) Basic EDA (brief)\n",
    "Keep this concise and focused on modeling decisions.\n",
    "\n",
    "**Suggested checks:**\n",
    "- Target balance (`buy`).  \n",
    "- Distributions of numeric features (e.g., `age`, `tenure`, `v_prem_quote`).  \n",
    "- Cardinality of `region`, `cs_rep`.  \n",
    "- Categorical value ranges (`gender`, `v_age`, `v_accident`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e9d8f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target and features\n",
    "y = df_train['buy'].astype(int)\n",
    "X = df_train.drop(columns=['buy'])\n",
    "\n",
    "\n",
    "# Identify feature types\n",
    "numeric_features = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=['int64','float64']).columns.tolist()\n",
    "\n",
    "\n",
    "X = X.drop(columns=['id_new'])\n",
    "score_ids = df_score['id_new'].copy()\n",
    "X_score = df_score.drop(columns=['id_new'])\n",
    "\n",
    "numeric_features = [c for c in X.select_dtypes(include=['int64','float64']).columns]\n",
    "categorical_features = [c for c in X.select_dtypes(exclude=['int64','float64']).columns]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('OneHotEncoder', OneHotEncoder(drop='first'), categorical_features),\n",
    "        ('StandardScaler', StandardScaler(with_mean=False), numeric_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fba3be",
   "metadata": {},
   "source": [
    "## 6) Train/Test Split\n",
    "Stratify on `buy` to preserve class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07c56b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b00ba",
   "metadata": {},
   "source": [
    "## 7) Preprocessing (Pipelines)\n",
    "Use a **ColumnTransformer** so the *same* steps can be reused for TEST and SCORE.\n",
    "\n",
    "**Notes:**\n",
    "- Treat high-cardinality IDs (e.g., `region`, `cs_rep`) with One-Hot (can be large) or try frequency encoding.\n",
    "- One-Hot encode: `gender`, `v_age`, `v_accident`, `region`, `cs_rep`.\n",
    "- Scale numeric features as needed for certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae4ddc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=preprocessor.fit_transform(X_train)\n",
    "X_test=preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f192455",
   "metadata": {},
   "source": [
    "## 8) Baseline Models\n",
    "Start with a few solid baselines and compare AUC/F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954dca8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ce15466",
   "metadata": {},
   "source": [
    "## 9) Model Selection & (Optional) Hyperparameter Tuning\n",
    "Pick the best baseline by AUC/F1, then optionally run a small grid search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206885ad",
   "metadata": {},
   "source": [
    "## 10) Fit Final Model on Full Training Set\n",
    "Use the chosen/tuned pipeline and refit on the entire TRAIN set (`X`, `y`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ff645",
   "metadata": {},
   "source": [
    "## 11) Score Dataset → Create `my_prediction.csv`\n",
    "Follow the required format: `id_new`, `probability` (for class 1 only), `classification` (argmax)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5c983",
   "metadata": {},
   "source": [
    "## 12) Results, Interpretation, and Recommendations\n",
    "**Summarize:**\n",
    "- Best model and *why* it was chosen.\n",
    "- AUC/F1 on the test set and what that implies.\n",
    "- Any key drivers of purchase you identified.\n",
    "- Business recommendations (who to target, how to use scores, next steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77dcadd",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "- Python/Sklearn versions\n",
    "- Reproducibility notes\n",
    "- Any references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e506adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, sklearn\n",
    "# print('Python:', sys.version)\n",
    "# print('pandas:', pd.__version__)\n",
    "# print('numpy:', np.__version__)\n",
    "# print('sklearn:', sklearn.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
