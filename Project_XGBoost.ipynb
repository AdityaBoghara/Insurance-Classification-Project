{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a394fa",
   "metadata": {},
   "source": [
    "# Final Course Group Project – Insurance Purchase Prediction\n",
    "\n",
    "**Course:** BZAN 6357 – Business Analytics with Python  \n",
    "**Project Type:** Supervised ML (Classification)  \n",
    "**Template generated:** 2025-10-30\n",
    "\n",
    "## Team\n",
    "- Aditya Boghara \n",
    "- Meghana\n",
    "\n",
    "## Deliverables\n",
    "Submit a single zip with:  \n",
    "1) This notebook (fully executed).  \n",
    "2) `my_prediction.csv` with **exactly** 3 columns: `id_new`, `probability`, `classification`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce3064",
   "metadata": {},
   "source": [
    "## 1) Introduction & Objective\n",
    "- **Background:** Cross-sell *car insurance* to existing medical policyholders.\n",
    "- **Objective:** Predict purchase probability (1=purchased, 0=not purchased) and classify Score data.\n",
    "- **Evaluation:** AUC-ROC and F1 score on held-out test; clarity and rigor of this notebook.\n",
    "- **Approach (summary):** Data prep → EDA → Modeling (baseline → tuned) → Evaluation → Score file export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc08fd9",
   "metadata": {},
   "source": [
    "## 2) Setup\n",
    "Fill in project constants and file paths if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a86f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Project constants ===\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2  # 20% test split\n",
    "N_FOLDS = 5  # 5- or 10-fold CV recommended\n",
    "\n",
    "# File names expected by the project\n",
    "TRAIN_FILE = \"bzan6357_insurance_3_TRAINING.csv\"\n",
    "SCORE_FILE = \"bzan6357_insurance_3_SCORE.csv\"\n",
    "SUBMIT_FILE = \"my_prediction.csv\"  # must contain: id_new, probability, classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d3367",
   "metadata": {},
   "source": [
    "## 3) Imports\n",
    "Only add libraries you actually use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd95ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, f1_score, roc_curve, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f304c9",
   "metadata": {},
   "source": [
    "## 4) Data Load & Quick Audit\n",
    "If files are missing, you'll see a helpful message instead of a crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1747790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy\n",
      "0    16705\n",
      "1     3755\n",
      "Name: count, dtype: int64\n",
      "buy\n",
      "0    81.647116\n",
      "1    18.352884\n",
      "Name: proportion, dtype: float64\n",
      "Shape of df_train (20460, 12)\n",
      "Shape of df_score (2000, 11)\n"
     ]
    }
   ],
   "source": [
    "# Load data (paths are already set above)\n",
    "df_train = pd.read_csv(TRAIN_FILE)\n",
    "df_score = pd.read_csv(SCORE_FILE)\n",
    "\n",
    "df_train.head()\n",
    "\n",
    "y = df_train[\"buy\"]\n",
    "\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"Shape of df_train\", df_train.shape)\n",
    "print(\"Shape of df_score\", df_score.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc063af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be43733d",
   "metadata": {},
   "source": [
    "## 5) Basic EDA (brief)\n",
    "Keep this concise and focused on modeling decisions.\n",
    "\n",
    "**Suggested checks:**\n",
    "- Target balance (`buy`).  \n",
    "- Distributions of numeric features (e.g., `age`, `tenure`, `v_prem_quote`).  \n",
    "- Cardinality of `region`, `cs_rep`.  \n",
    "- Categorical value ranges (`gender`, `v_age`, `v_accident`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9d8f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'tenure', 'region', 'dl', 'has_v_insurance', 'v_prem_quote', 'cs_rep']\n",
      "['gender', 'v_age', 'v_accident']\n",
      "Shape of df_train (20460, 10)\n",
      "Shape of df_score (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Target and features\n",
    "y = df_train[\"buy\"].astype(int)\n",
    "X = df_train.drop(columns=[\"buy\"])\n",
    "\n",
    "\n",
    "X = X.drop(columns=[\"id_new\"])\n",
    "\n",
    "\n",
    "# Identify feature types\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "\n",
    "print(numeric_features)\n",
    "print(categorical_features)\n",
    "\n",
    "\n",
    "score_ids = df_score[\"id_new\"].copy()\n",
    "X_score = df_score.drop(columns=[\"id_new\"])\n",
    "\n",
    "numeric_features = [c for c in X.select_dtypes(include=[\"int64\", \"float64\"]).columns]\n",
    "categorical_features = [\n",
    "    c for c in X.select_dtypes(exclude=[\"int64\", \"float64\"]).columns\n",
    "]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"OneHotEncoder\", OneHotEncoder(drop=\"first\"), categorical_features),\n",
    "        (\"StandardScaler\", StandardScaler(with_mean=False), numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Shape of df_train\", X.shape)\n",
    "print(\"Shape of df_score\", X_score.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b00ba",
   "metadata": {},
   "source": [
    "## 6) Preprocessing (Pipelines)\n",
    "Use a **ColumnTransformer** so the *same* steps can be reused for TEST and SCORE.\n",
    "\n",
    "**Notes:**\n",
    "- Treat high-cardinality IDs (e.g., `region`, `cs_rep`) with One-Hot (can be large) or try frequency encoding.\n",
    "- One-Hot encode: `gender`, `v_age`, `v_accident`, `region`, `cs_rep`.\n",
    "- Scale numeric features as needed for certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4ddc5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , ..., 0.        , 1.39816747,\n",
       "        9.7316272 ],\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 2.3758078 ,\n",
       "        9.19429809],\n",
       "       [1.        , 0.        , 0.        , ..., 2.23570222, 1.64180453,\n",
       "        7.28379459],\n",
       "       ...,\n",
       "       [1.        , 0.        , 0.        , ..., 2.23570222, 3.62885928,\n",
       "        7.28379459],\n",
       "       [1.        , 0.        , 0.        , ..., 0.        , 1.95069579,\n",
       "        7.28379459],\n",
       "       [1.        , 0.        , 1.        , ..., 0.        , 2.10455499,\n",
       "        7.28379459]], shape=(2000, 11))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = preprocessor.fit_transform(X)\n",
    "X_score = preprocessor.transform(X_score)\n",
    "\n",
    "X_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fba3be",
   "metadata": {},
   "source": [
    "## 7) Train/Test Split\n",
    "Stratify on `buy` to preserve class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c56b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de0de910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: (16368, 11) (16368,)\n",
      "After SMOTE: (26786, 11) (26786,)\n",
      "value count before SMOTE: buy\n",
      "0    13393\n",
      "1     2975\n",
      "Name: count, dtype: int64\n",
      "value count After SMOTE: buy\n",
      "0    13393\n",
      "1    13393\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 2. Apply SMOTE only on the training set\n",
    "sm = SMOTE(random_state=42)\n",
    "X_tr_res, y_tr_res = sm.fit_resample(X_tr, y_tr)\n",
    "\n",
    "\n",
    "print(\"Before SMOTE:\", X_tr.shape, y_tr.shape)\n",
    "print(\"After SMOTE:\", X_tr_res.shape, y_tr_res.shape)\n",
    "\n",
    "print(\"value count before SMOTE:\", y_tr.value_counts())\n",
    "print(\"value count After SMOTE:\", y_tr_res.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f192455",
   "metadata": {},
   "source": [
    "## 8) Baseline Models\n",
    "Start with a few solid baselines and compare AUC/F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c031a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost Regressor\n",
      "Confusion Matrix (val):\n",
      " [[11545  1848]\n",
      " [ 1158 12235]]\n",
      "\n",
      "Classification Report fot train (val):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9088    0.8620    0.8848     13393\n",
      "           1     0.8688    0.9135    0.8906     13393\n",
      "\n",
      "    accuracy                         0.8878     26786\n",
      "   macro avg     0.8888    0.8878    0.8877     26786\n",
      "weighted avg     0.8888    0.8878    0.8877     26786\n",
      "\n",
      "accuracy:  0.8877771970432315\n",
      "----------------------------------\n",
      "Confusion Matrix (val):\n",
      " [[14114  2591]\n",
      " [ 1267  2488]]\n",
      "\n",
      "Classification Repor for test (val):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9176    0.8449    0.8798     16705\n",
      "           1     0.4899    0.6626    0.5633      3755\n",
      "\n",
      "    accuracy                         0.8114     20460\n",
      "   macro avg     0.7037    0.7537    0.7215     20460\n",
      "weighted avg     0.8391    0.8114    0.8217     20460\n",
      "\n",
      "accuracy:  0.8114369501466275\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "## Beginning Model Training\n",
    "models = {\n",
    "    \"Xgboost Regressor\":XGBClassifier()\n",
    "   \n",
    "}\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_tr_res, y_tr_res) # Train model\n",
    "\n",
    "    # Make prediction\n",
    "\n",
    "    # y_train_pred = model.predict(X_tr_res)\n",
    "    y_proba_tr = model.predict_proba(X_tr_res)[:, 1]\n",
    "    y_train_pred = (y_proba_tr >= 0.5).astype(int)\n",
    "    y_test_pred = model.predict(X)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(list(models.keys())[i])\n",
    "    \n",
    "    print(\"Confusion Matrix (val):\\n\", confusion_matrix(y_tr_res, y_train_pred))\n",
    "    print(\n",
    "    \"\\nClassification Report fot train (val):\\n\", classification_report(y_tr_res, y_train_pred,  digits=4)\n",
    "    )\n",
    "    \n",
    "    print(\"accuracy: \", accuracy_score(y_tr_res, y_train_pred))\n",
    "\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    print(\"Confusion Matrix (val):\\n\", confusion_matrix(y, y_test_pred))\n",
    "    print(\n",
    "    \"\\nClassification Repor for test (val):\\n\", classification_report(y, y_test_pred, digits=4)\n",
    "    )\n",
    "    print(\"accuracy: \", accuracy_score(y, y_test_pred))\n",
    "    \n",
    "    print('='*35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce15466",
   "metadata": {},
   "source": [
    "## 9) Model Selection & (Optional) Hyperparameter Tuning\n",
    "Pick the best baseline by AUC/F1, then optionally run a small grid search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206885ad",
   "metadata": {},
   "source": [
    "## 10) Fit Final Model on Full Training Set\n",
    "Use the chosen/tuned pipeline and refit on the entire TRAIN set (`X`, `y`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ff645",
   "metadata": {},
   "source": [
    "## 11) Score Dataset → Create `my_prediction.csv`\n",
    "Follow the required format: `id_new`, `probability` (for class 1 only), `classification` (argmax)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5c983",
   "metadata": {},
   "source": [
    "## 12) Results, Interpretation, and Recommendations\n",
    "**Summarize:**\n",
    "- Best model and *why* it was chosen.\n",
    "- AUC/F1 on the test set and what that implies.\n",
    "- Any key drivers of purchase you identified.\n",
    "- Business recommendations (who to target, how to use scores, next steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77dcadd",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "- Python/Sklearn versions\n",
    "- Reproducibility notes\n",
    "- Any references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e506adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:07:49) [Clang 20.1.8 ]\n",
      "pandas: 2.3.3\n",
      "numpy: 2.3.4\n",
      "sklearn: 1.7.2\n",
      "XGBoost: 3.1.1\n"
     ]
    }
   ],
   "source": [
    "import sys, sklearn, xgboost\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"XGBoost:\", xgboost.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13261fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fef6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d1057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
